[
{
	"uri": "http://localhost:1313/fcj-workshop2/5-workshop/5.1-workshop-overview/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "DevSecOps Pipeline Overview (GitLab → AWS) Pipeline goals: Automate the entire build process – security scanning – error notification. Detect bugs and security vulnerabilities early as soon as dev commits code. Create a closed DevSecOps loop: Commit → Scan → Notify → Fix → Commit again.\nFlow summary: Dev commits code to GitLab (main branch). AWS CodePipeline receives events and triggers the pipeline. CodeBuild runs Sonar Scanner to analyze the source code. SonarQube on EC2 receives scan results from Scanner. SonarQube sends Webhook → API Gateway → Lambda. Lambda processes data → sends notification via SNS → dev email. Dev receives bug report → Fix → Commit → go back to loop. The system ensures: Automate security testing. Improve source code quality. Reduce security risks and logic errors. "
},
{
	"uri": "http://localhost:1313/fcj-workshop2/1-worklog/",
	"title": "Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1: Getting familiar with AWS and basic AWS services\nWeek 2: Doing task A\u0026hellip;\nWeek 3: Doing task B\u0026hellip;\nWeek 4: Doing task C\u0026hellip;\nWeek 5: Doing task D\u0026hellip;\nWeek 6: Doing task E\u0026hellip;\nWeek 7: Doing task G\u0026hellip;\nWeek 8: Doing task H\u0026hellip;\nWeek 9: Doing task I\u0026hellip;\nWeek 10: Doing task L\u0026hellip;\nWeek 11: Doing task M\u0026hellip;\nWeek 12: Doing task N\u0026hellip;\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop2/4-eventparticipated/4.1-event1/",
	"title": "Event 1",
	"tags": [],
	"description": "",
	"content": "Workshop Report: \u0026ldquo;AI/ML/GenAI on AWS\u0026rdquo; Workshop Objectives Gain an overview of the AI/ML landscape and how AWS is adopted in Vietnam Understand the full ML workflow using Amazon SageMaker Explore Generative AI capabilities through Amazon Bedrock Practice prompt engineering and RAG (Retrieval-Augmented Generation) Learn how to build practical AI/ML solutions using AWS services Event Information Location: AWS Vietnam Office Time: 8:30 AM – 12:00 PM, Saturday, November 15, 2025 Speakers \u0026amp; Organizing Team Instructors:\nLâm Tuấn Kiệt – Senior DevOps Engineer, FPT Software Presented the SageMaker overview and general AWS ML services Đinh Lê Hoàng Anh – Cloud Engineer Trainee, FCAJ, Swinburne University of Technology Covered Amazon Bedrock and AWS AI/ML service ecosystem Danh Hoàng Hiếu Nghị – Fresher AI Engineer, Renova Cloud Demonstrated Bedrock Agent Core and guided the hands-on practice Coordinators:\nAWS Vietnam Community Team FCJ (First Cloud Journey) Program Leads Agenda Overview 8:30 – 9:00 AM: Registration \u0026amp; Kickoff Participant check-in and early networking Introduction to the workshop structure and expected outcomes Ice-breaker session A quick look at the AI/ML market and adoption trends in Vietnam 9:00 – 10:30 AM: AWS AI/ML Overview – Deep Dive into Amazon SageMaker Amazon SageMaker – The End-to-End ML Platform\nData Preparation \u0026amp; Labeling:\nData Wrangler for cleaning and transforming datasets Ground Truth for labeling and annotation Feature Store for centralizing and reusing features Training, Tuning \u0026amp; Deployment:\nUse built-in algorithms or bring your own training scripts Run hyperparameter tuning jobs Deploy models via real-time, batch, or serverless inference Experiment with A/B testing models and multi-model endpoints Built-in MLOps Capabilities:\nSageMaker Pipelines to automate ML workflows Model Registry for tracking versions and governance Model Monitor to detect drift and quality issues CI/CD integration for continuous deployment Live Demo – SageMaker Studio:\nLaunch a notebook instance Train a sample ML model Deploy an endpoint and send test requests 10:30 – 10:45 AM: Coffee Break Short break for refreshments Casual Q\u0026amp;A with AWS engineers 10:45 AM – 12:00 PM: Generative AI with Amazon Bedrock \u0026amp; AWS AI/ML Services AWS AI/ML Services Overview\nRekognition – Image/video analysis Translate – Neural machine translation Textract – Document text extraction Transcribe – Speech-to-text Polly – Natural-sounding text-to-speech Comprehend – NLP and text analytics Kendra – Intelligent search engine Lookout – Industrial anomaly detection Personalize – ML-powered recommendations Foundation Models: Claude, Llama, Titan\nModel Selection Insights: Claude: Strong in reasoning-heavy conversation tasks Llama: Good for customization, open-source flexibility Titan: Amazon-native, cost-effective, integrated with AWS How to choose depending on use case Prompt Engineering Essentials\nCore Prompting Techniques:\nProvide clear instructions and meaningful context Use few-shot examples Apply chain-of-thought for complex logic Use role-based prompting to guide model behavior Advanced Prompting:\nTuning temperature and token limits Knowing when to use system vs user prompts Template-based prompting for reusability Retrieval-Augmented Generation (RAG)\nRAG Architecture:\nEmbeddings + vector search Semantic retrieval before generating answers Feeding retrieved context back into prompts Knowledge Base Integration:\nBedrock Knowledge Bases Store documents in Amazon S3 Connect to data sources (S3, DBs, external APIs) Best practices for chunking and metadata Set correct bucket policies for secure access Amazon Bedrock Agent Core\nBuilding Autonomous Agents:\nMulti-step task planning and orchestration Action groups for calling APIs Persistent memory for context retention Tool Integration with Lambda:\nLambda as an execution engine for custom logic Real-time data processing Querying databases or external systems Serverless approach → simplified scaling and maintenance Guardrails for Safe AI\nContent filtering and moderation PII detection and redaction Topic restrictions and safety rules Custom guardrails for enterprise policies Live Demo – Creating a GenAI Chatbot with Bedrock\nEnabling foundation model access Designing prompts for a basic chatbot Adding RAG with Knowledge Bases Applying guardrails to control responses Testing and fine-tuning the bot Key Insights From Amazon SageMaker A complete platform that covers the entire ML lifecycle Well-integrated MLOps tools Easy scaling from experimentation to production Flexible pricing models to control cost From Amazon Bedrock Variety of foundation models readily available Prompt engineering plays a major role in result quality RAG helps integrate enterprise knowledge effectively Guardrails are essential for safe deployment Agent Core enables multi-step intelligent workflows Practical Lessons Start with the business problem, not the tool Prototype quickly using SageMaker Studio Use foundation models before investing in custom training Guardrails ensure safety and compliance Always monitor performance and optimize workloads Applying the Knowledge Experiment in SageMaker Studio using small datasets Build a RAG-based chatbot using Bedrock + S3 Practice prompt engineering with different models Automate ML pipelines with SageMaker Pipelines Try building simple Bedrock Agents for internal workflows Implement guardrails before deploying any AI app Share learnings with teammates to improve team-wide standards Personal Experience Attending the “AI/ML/GenAI on AWS Workshop” at the AWS Vietnam Office was a hands-on and insightful experience. The mix of explanations, demos, and real-world examples made the content easier to understand.\nLearning from AWS Experts Clear explanations on SageMaker’s end-to-end workflow Solid demos of Bedrock and real GenAI applications Many Vietnam-based use cases provided good context Practical advice on choosing the right AWS tools Hands-on Demonstrations Observed the full ML process: data → training → deployment Learned how Bedrock simplifies building GenAI applications Applied prompt engineering tricks in real examples Understood how RAG makes LLMs more accurate with enterprise data Saw how Agents organize multi-step tasks Understanding AI/ML Trends Differences between traditional ML and GenAI became clearer Better idea of when to pick SageMaker vs Bedrock Understood the importance of proper MLOps for production systems Networking Met many developers and data enthusiasts exploring AWS AI/ML Exchanged experiences about real-world implementation challenges Built new connections with AWS community members Important Takeaways Foundation models significantly reduce development effort Good prompts dramatically improve model output RAG is essential for knowledge-heavy chatbot applications Guardrails are not optional—they are required for safe AI SageMaker is ideal for long-term, scalable ML projects Next Steps Continue exploring SageMaker Studio hands-on Build a small RAG proof-of-concept using Bedrock Practice prompt engineering with multiple foundation models Experiment with Bedrock Agents for automation workflows Learn more about MLOps and monitoring practices Engage with the AWS AI/ML community for ongoing learning Event Pictures Overall, this workshop provided a comprehensive introduction to AWS AI/ML services, from traditional machine learning with SageMaker to cutting-edge Generative AI with Bedrock. The hands-on demonstrations and expert guidance made complex concepts accessible and immediately applicable. The key takeaway is that AWS provides a complete ecosystem for building, deploying, and scaling AI/ML applications, making it easier than ever to bring AI innovations to production.\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop2/3-blogstranslated/3.1-blog1/",
	"title": "Using Large Language Models on Amazon Bedrock for Multi-step Task Execution",
	"tags": [],
	"description": "",
	"content": "This article explains how to use Large Language Models (LLMs) on Amazon Bedrock to carry out analytical tasks that require multi-step reasoning and external APIs. The goal is to convert complex questions into a structured Plan → Execute workflow that is predictable and scalable.\n1. Background Business analytics questions are typically complex, such as:\n“What is the average hospital stay for patients with a specific condition across hospitals?” “How do prescription trends for a particular medication differ across regions?” Traditional solutions require BI experts and data engineers.\nWith LLMs + tools, we can automate and accelerate these workflows.\n2. Tools in the LLM Context Tools are external capabilities the LLM can call, such as:\nAPIs for real-time data Functions for computation or data processing Filtering, grouping, or joining data Using tools allows the LLM to produce accurate, contextual, and actionable outputs.\n3. Example Interaction User: “Who is the patient with the lowest number of vaccines?”\nAI: “The patient is Sharleen176 Kulas532 with 1 vaccine.”\nSteps performed:\nRetrieve patient data Retrieve immunization data Group by patient Count vaccines Sort ascending Select top result Join with patient details 4. Dataset \u0026amp; Setup The system uses the Synthetic Patient Generation Dataset, containing multiple healthcare tables.\nSetup involves downloading, extracting, and placing data in the project folder.\n5. Solution Architecture: Plan → Execute Two-phase workflow:\nPlan: LLM generates a step-by-step plan Execute: Engine executes each step Flow:\nUser → LLM Plan → JSON Plan → Execution Engine → Answer\n6. Planning Phase Why Planning? The LLM:\nPerforms step-by-step reasoning Produces a structured workflow Avoids hallucinated API calls Tools are provided as function signatures the LLM can use.\nRAG for Tool Selection RAG helps show the LLM only relevant tools, improving accuracy and reducing complexity.\nExample Plan For the query “Find the patient with the fewest vaccines,” the plan includes:\nRetrieve patients Retrieve immunizations Group by patient Count Sort Limit Join Select 7. Execution Phase The engine takes the JSON plan and:\nParses it Executes each function Stores intermediate results Returns the final answer The LLM then frames the output into a natural-language response.\n8. Error Handling Possible failures:\nEmpty or missing data Invalid parameters Type mismatches Engine responsibilities:\nValidate inputs Validate outputs Return meaningful errors The LLM may regenerate a better plan automatically.\n9. Summary We explored:\nHow LLMs use APIs to answer complex questions The Plan → Execute architecture The role of RAG and function signatures Error handling in execution LLMs can now act as orchestration brains for analytical workflows.\n10. Future Improvements Extensions include:\nMore analytical questions Additional tool signatures A web UI for question input, plan visualization, and execution logs This makes the project both academically strong and practically relevant.\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop2/",
	"title": "Internship Report",
	"tags": [],
	"description": "",
	"content": "Internship Report Student Information: Full Name: Le Minh Duong\nPhone Number: 0347622638\nEmail: leduong5469@gmail.com\nUniversity: FPT University\nMajor: Information security\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 08/09/2025 to 12/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "
},
{
	"uri": "http://localhost:1313/fcj-workshop2/1-worklog/1.1-week1/",
	"title": "Week 1 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Create an AWS account. Watch video tutorials on installing Hugo, Git, VScode. Learn how to write markdown, learn how to use hugo. Do labs on the First Cloud Journey Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 Get acquainted with FCJ membersRead and take note of internship unit rules and regulations 08/09/2025 08/09/2025 3 Learn about AWS and its types of services:ComputeStorageNetworkingDatabase\u0026hellip; 09/09/2025 09/09/2025 CloudJourney 4 Create AWS Free Tier accountLearn about AWS Console \u0026amp; AWS CLIPractice:Create AWS accountInstall \u0026amp; configure AWS CLIHow to use AWS CLI 10/09/2025 10/09/2025 AWS Account Setup 5 Learn how to write markdown and use Hugo 11/09/2025 11/09/2025 YouTube Guide 6 Practice:Do labs on the First Cloud Journey 12/09/2025 12/09/2025 First Cloud Journey Week 1 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region successfully installed hugo and git and VScode:\nknow how to use hugo and git and VScode know how to write markdown and write a successful worklog week 1.\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop2/5-workshop/5.2-pipline-flow/",
	"title": "Pipline-flow",
	"tags": [],
	"description": "",
	"content": "CodePipeline Flow (GitLab → AWS) 1. Source Stage CodePipeline is configured to listen for commits from GitLab (branch: main).\nWhen there is a new commit, the pipeline is automatically activated.\nOutput: source artifact to transfer to the build step. 2. Build Stage (CodeBuild) Run the build environment according to buildspec.yml.\nExecute:\nInstall Sonar Scanner.\nScan all source code.\nSend results to SonarQube Server.\nIf the build fails → the pipeline stops.\n3. Post-Build Actions CodeBuild pushes build metadata (log, status) to CodePipeline.\nNotify optional: Amazon EventBridge / SNS / Slack.\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop2/4-eventparticipated/4.2-event2/",
	"title": "Event 2",
	"tags": [],
	"description": "",
	"content": "Workshop Report: \u0026ldquo;DevOps on AWS\u0026rdquo; Event Objectives Get a clear understanding of DevOps culture, principles, and how it changes the way teams work Learn how to build CI/CD pipelines using AWS DevOps services Practice Infrastructure as Code with AWS CloudFormation and the AWS CDK Explore container platforms on AWS: ECR, ECS, EKS, and App Runner Set up monitoring and observability with Amazon CloudWatch and AWS X-Ray See how DevOps practices are applied in real production environments Event Details Location: AWS Vietnam Office Date \u0026amp; Time: 8:30 AM – 5:00 PM, Monday, November 17, 2025 Speakers \u0026amp; Facilitators Instructors:\nTruong Quang Tinh – AWS Community Builder DevOps culture and CI/CD fundamentals Van Hoang Kha – AWS Community Builder Infrastructure as Code with CloudFormation Nguyen Khanh Phuc Thinh – AWS Community Builder AWS CDK deep dive Le Huynh Nghiem – AWS Community Builder Container services on AWS Huynh Hoang Long – AWS Community Builder Monitoring and observability Pham Hoang Quy – AWS Community Builder DevOps best practices and real-world stories Facilitators:\nAWS Vietnam Team AWS Community Builders Vietnam Event Agenda 8:30 AM – 9:00 AM: Registration and Opening Check-in and informal networking Welcome talk and overview of the workshop flow Short introduction to key AWS DevOps services 9:00 AM – 10:30 AM: DevOps Culture and CI/CD Pipeline Speaker: Truong Quang Tinh\nDevOps Mindset – shifting from “Dev vs Ops” to “one team”:\nCollaboration: Dev and Ops work together instead of separately Automation: Let tools handle repetitive work so teams can focus on value Continuous Improvement: Use feedback to adjust and improve over time Shared Ownership: Quality, security, and reliability are everyone’s job DORA Metrics – how to know if DevOps is working:\nDeployment Frequency – how often you push changes live Lead Time for Changes – time from commit to production Mean Time to Recovery (MTTR) – how fast you recover from incidents Change Failure Rate – how many releases cause problems Core CI/CD Services on AWS:\nAWS CodeCommit – Git repositories hosted on AWS AWS CodeBuild – build and test automation service AWS CodeDeploy – roll out changes to EC2, Lambda, ECS, or on-prem servers AWS CodePipeline – main pipeline that ties all stages together Deployment Strategies:\nBlue/Green – switch traffic between two environments to reduce downtime Canary – release changes to a small portion of users first Rolling – update instances gradually so the service stays available Live Demo:\nSet up a basic CI/CD pipeline end-to-end using AWS DevOps tools 10:30 AM – 10:45 AM: Coffee Break Short break for coffee and networking 10:45 AM – 12:00 PM: Infrastructure as Code with CloudFormation Speaker: Van Hoang Kha\nCore Ideas of Infrastructure as Code (IaC):\nVersion Control – treat infrastructure like code stored in Git Repeatability – spin up the same environment again and again Living Documentation – templates describe what exists in the cloud Pre-deployment Testing – validate before roll-out CloudFormation Basics:\nTemplates – YAML/JSON files that describe AWS resources Stacks – a group of resources created and managed together Change Sets – preview what will change before you apply it Drift Detection – detect manual changes that differ from the template Best Practices with CloudFormation:\nModular Design – use nested stacks to reuse patterns Parameters – keep templates flexible with input values Outputs – expose useful values for other stacks or teams Cross-Stack References – connect multiple stacks cleanly Advanced Concepts:\nStack Policies – prevent accidental changes to critical pieces Rollback Triggers – roll back when CloudWatch alarms are triggered StackSets – roll out stacks to multiple accounts/regions at once Live Demo:\nDeploy a simple multi-tier app using CloudFormation templates 12:00 PM – 1:00 PM: Lunch Break Lunch and informal discussion with speakers and participants 1:00 PM – 2:15 PM: AWS CDK Deep Dive Speaker: Nguyen Khanh Phuc Thinh\nWhat is AWS CDK?\nInfrastructure described with real programming languages Supports TypeScript, Python, Java, C#, Go, and more Different abstraction layers: L1 – direct CloudFormation mapping L2 – higher-level constructs L3 – ready-made patterns CDK Building Blocks:\nConstructs – reusable building units for cloud apps Stacks – deployment units made of constructs Apps – a collection of stacks deployable as one project Synthesis – CDK code → CloudFormation template CDK vs CloudFormation:\nImperative vs Declarative – use loops, conditions, functions in CDK Type Safety – catch mistakes early via the compiler IDE Support – autocompletion and inline docs help productivity Testing – write tests for your infrastructure code CDK Good Practices:\nUse and share construct libraries Design stacks that are environment-agnostic when possible Keep logical IDs stable to avoid unnecessary replacements Use context values for per-environment configuration Live Demo:\nBuild and deploy a small serverless app using AWS CDK 2:15 PM – 2:30 PM: Coffee Break Short break and more networking 2:30 PM – 3:45 PM: Container Services on AWS Speaker: Le Huynh Nghiem\nContainer Basics:\nWhat containers are and why they’re popular Benefits: consistent environments, portability, resource efficiency Quick review of Docker concepts: images, containers, registries Amazon Elastic Container Registry (ECR):\nPrivate container image registry managed by AWS Built-in image scanning for vulnerabilities Lifecycle policies to auto-delete old images Cross-region replication to speed up deployments globally Amazon Elastic Container Service (ECS):\nAWS-native container orchestration service Task Definitions – recipe for running containers Services – define desired count and keep tasks running Fargate – run containers without managing servers EC2 Launch Type – full control of the underlying instances Amazon Elastic Kubernetes Service (EKS):\nManaged control plane for Kubernetes clusters Use standard Kubernetes APIs and tools Flexible enough for complex microservice architectures Integration with AWS components like ALB, EBS, EFS AWS App Runner:\nSimplest way to go from source code or container image to a running service Handles scaling automatically Integrates with CI/CD flows Ideal for web apps and APIs that don’t need complex orchestration Choosing the Right Container Service:\nECS – good default choice for AWS-focused teams EKS – for teams already familiar with Kubernetes App Runner – when speed and simplicity are top priority Live Demo:\nDeploy a container-based application to ECS and EKS 3:45 PM – 4:45 PM: Monitoring and Observability Speaker: Huynh Hoang Long\nMonitoring vs Observability:\nMonitoring – collecting metrics and checking if things are OK Observability – being able to understand internal behavior from external outputs Focus on the three pillars: logs, metrics, traces Amazon CloudWatch:\nMetrics – track CPU, memory (custom), and app metrics Logs – central place to store and search logs Alarms – trigger alerts or actions based on thresholds Dashboards – custom views for system health Logs Insights – query logs with a SQL-like syntax Events – react to changes in your AWS environment AWS X-Ray:\nDistributed tracing – follow a request across multiple services Service map – visual view of how services talk to each other Trace analysis – identify slow parts and bottlenecks Error analysis – pinpoint where failures happen Works with Lambda, ECS, EKS, API Gateway, and more Good Practices for Observability:\nUse structured logging (JSON or consistent formats) Add custom metrics for business-level KPIs Configure alerts before users notice issues Correlate logs, metrics, and traces for faster debugging Apply retention policies to balance visibility and cost Live Demo:\nSet up dashboards, alarms and tracing for a small microservices system 4:45 PM – 5:00 PM: DevOps Best Practices \u0026amp; Q\u0026amp;A Speaker: Pham Hoang Quy\nDevOps Best Practices:\nAutomate as much as possible: tests, builds, deployments, checks Fail fast and learn: treat incidents as learning opportunities Blameless postmortems: focus on process, not on blaming people Progressive delivery: feature flags, canary, blue/green releases Continuous learning: DevOps is an ongoing improvement cycle Case Studies:\nStartup – use serverless and CI/CD to deliver features quickly Enterprise – multi-account setups with CloudFormation StackSets E-commerce – high-availability deployments using blue/green Q\u0026amp;A Session:\nOpen discussion with participants about tools, culture, and real problems Key Takeaways DevOps Culture and Mindset DevOps is about people, collaboration, and shared goals, not just tools Automation is key to reducing manual work and mistakes DORA metrics give a concrete way to see how the team is improving Continuous improvement is at the heart of DevOps Quality and reliability are shared responsibilities across the team CI/CD Pipeline Automation CodeCommit, CodeBuild, CodeDeploy, and CodePipeline work well together End-to-end automation improves speed and reduces human error Multiple deployment patterns (blue/green, canary, rolling) support safer releases CI/CD can integrate with both AWS-native and third-party services A good pipeline becomes the backbone of the delivery process Infrastructure as Code CloudFormation templates provide a clear and repeatable infrastructure definition CDK adds a more developer-friendly way to define the same resources IaC improves consistency, traceability, and collaboration Drift detection helps catch manual edits that break the IaC model Choosing between CloudFormation and CDK depends on team skills and preference Container Services ECR secures and manages container images with scanning and policies ECS is a strong default for many AWS-based container workloads EKS is powerful for teams already invested in Kubernetes App Runner is ideal when you want “code → service” with minimal steps The right container platform depends on complexity, control, and team experience Monitoring and Observability CloudWatch and X-Ray provide a complete toolkit for visibility on AWS Observability helps understand why something went wrong, not just that it did Proactive alerts are essential for minimizing downtime Logs, metrics, and traces together give a full picture of system health Data from monitoring can guide performance tuning and cost optimizations DevOps Best Practices Automation + culture = effective DevOps, tools alone are not enough Learning from incidents is more important than avoiding them at all costs Progressive delivery reduces risk for every release Regularly reviewing processes and metrics keeps the team moving forward Applying to Work Start with a simple CI/CD pipeline using CodePipeline and expand from there Migrate manual setups into CloudFormation or CDK for consistency Gradually containerize applications and choose ECS/EKS/App Runner where appropriate Strengthen monitoring and observability with CloudWatch dashboards and X-Ray Encourage a DevOps culture of transparency, ownership, and continuous learning Use DORA metrics to track improvement instead of gut feeling Consider aiming for the AWS DevOps Engineer certification as a longer-term goal Event Experience The “DevOps on AWS Workshop” was a full-day, very packed but valuable experience. It covered not only the technical tools, but also the mindset and real-world practices behind DevOps.\nLearning from AWS Experts The speakers provided both high-level concepts and concrete examples Live demos made the theory easier to follow and remember Real case studies helped connect the tools to realistic use cases There was plenty of time to ask specific questions about real problems Hands-on Demonstrations Saw a complete CI/CD pipeline being assembled step by step Watched how IaC with CloudFormation and CDK works in practice Learned how to deploy containers on ECS and EKS Understood what proper monitoring and tracing look like in a microservices setup Understanding DevOps in Practice DevOps was presented as a combination of culture + process + tools DORA metrics provided a clear way to talk about performance and improvement Observability was highlighted as a core requirement for modern systems Examples of incident handling and postmortems were particularly useful Networking and Community Met many engineers who are on the same DevOps journey Shared common struggles like legacy systems, manual processes, and resistance to change Got connected to the AWS DevOps community for future events and learning Personal Takeaways Good DevOps requires both mindset change and technical skills Automation frees teams from “busy work” and lets them focus on real problems Having IaC and proper monitoring in place makes scaling much easier AWS provides almost everything needed to implement DevOps end-to-end Next Steps Build or improve an internal CI/CD pipeline using the ideas from the workshop Refactor existing infrastructure into CloudFormation/CDK where possible Plan a gradual move towards containers for suitable workloads Invest time in setting up meaningful dashboards and alerts Keep learning from AWS documentation, workshops, and the community "
},
{
	"uri": "http://localhost:1313/fcj-workshop2/3-blogstranslated/3.2-blog2/",
	"title": "Updated Carbon Methodology for the AWS Customer Carbon Footprint Tool (CCFT)",
	"tags": [],
	"description": "",
	"content": "(Based on the official AWS update from April 23, 2025)\nTo support customers on their sustainability journey, AWS launched the Customer Carbon Footprint Tool (CCFT) in 2022. CCFT helps customers track and evaluate carbon emissions resulting from their AWS usage. It includes Scope 1 and Scope 2 emissions according to the Greenhouse Gas Protocol, covering all AWS services such as Amazon EC2, Amazon S3, AWS Lambda, and more.\nToday, AWS announces three updates to CCFT:\nEasier access to carbon data through Billing and Cost Management Data Exports. Detailed carbon data broken down by AWS Region. An updated allocation methodology (v2.0), independently assured by APEX. Starting January 2025, CCFT uses methodology v2.0. Data from December 2024 and earlier will continue using v1.0.\n1. Easier Data Access Customers can now export CCFT carbon data via AWS Data Exports.\nKey features:\nProvides emission estimates for all accounts under AWS Organizations. Monthly automated exports delivered to Amazon S3 in CSV or Parquet. First export includes up to 38 months of historical data. Pre-Dec 2024 data uses v1.0; January 2025 onward uses v2.0.\n2. Regional Carbon Granularity Customers can now view carbon emissions broken down by AWS Region.\nCloudFront usage is grouped under Global Services.\nThis enhancement allows customers to:\nIdentify regions contributing the most carbon emissions Make better workload placement decisions 3. Updated Methodology v2.0 Customers often use services across many AWS Regions, making carbon attribution complex.\nMethodology v2.0 is aligned with industry standards including:\nGHG Protocol Corporate Standard GHG Protocol Product Standard ISO 14040/44 (Life Cycle Assessment) ISO 14067 (Product Carbon Footprint) ICT Sector Guidance Scope 1 Overview Scope 1 includes direct emissions from AWS-owned or controlled sources such as backup generators.\nAWS collects annual Scope 1 operational data, calculates emissions at site level, then aggregates them into clusters (AWS Regions or CloudFront edge clusters).\nScope 2 Overview Scope 2 includes indirect emissions from purchased electricity.\nCCFT uses:\nMarket-based methodology Geographic emission factors Grid mix and carbon intensity values verified annually It follows the prioritization rules of the Greenhouse Gas Protocol.\nAllocation Model v2.0 Emission allocation follows three steps:\nAllocate cluster-level emissions to server racks. Allocate rack emissions to AWS services based on resource usage and service dependencies. Allocate service emissions to individual customer accounts. Some customers may see changes in their totals due to improved accuracy.\nThree Key Updates in Methodology v2.0 Unused capacity is now allocated to all AWS customers.\nCarbon associated with unused server capacity is distributed proportionally, as required by GHG Protocol and ISO standards.\nImproved allocation logic for services without dedicated hardware, such as AWS Lambda or Amazon Redshift.\nUpdated allocation of shared overhead, including networking racks and AWS Region expansions.\nMoving Forward AWS will continue improving CCFT as new climate science, data, and customer needs evolve.\nClimate Pledge Commitment AWS remains committed to reaching net-zero carbon by 2040.\nTo learn more, visit the AWS sustainability site.\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop2/2-proposal/",
	"title": "Proposal",
	"tags": [],
	"description": "",
	"content": "AWS Security Scan Project – Project Plan [Team DevSecOps FCJ] – [FPT University / Internship Program] – [AWS Security Scan Project] Date: 2025-10-11\nTABLE OF CONTENTS 1.BACKGROUND AND MOTIVATION\n1.1 EXECUTIVE SUMMARY\n1.2 PROJECT SUCCESS CRITERIA\n1.3 ASSUMPTIONS\n2.SOLUTION ARCHITECTURE / ARCHITECTURAL DIAGRAM\n2.1 TECHNICAL ARCHITECTURE DIAGRAM\n2.2 TECHNICAL PLAN\n2.3 PROJECT PLAN\n2.4 SECURITY CONSIDERATIONS\n3.ACTIVITIES AND DELIVERABLES\n3.1 ACTIVITIES AND DELIVERABLES\n3.2 OUT OF SCOPE\n3.3 PATH TO PRODUCTION\n4.EXPECTED AWS COST BREAKDOWN BY SERVICES\n5.TEAM\n6.RESOURCES \u0026amp; COST ESTIMATES\n7.ACCEPTANCE\n1. BACKGROUND AND MOTIVATION 1.1 EXECUTIVE SUMMARY The AWS Security Scan Project aims to automate the security inspection process across the software development lifecycle by integrating AWS services such as CodePipeline, CodeBuild, CodeGuru Reviewer, and AWS Security Hub. This initiative enhances the security posture of continuous integration and deployment pipelines by embedding automated vulnerability scanning, AI-powered code analysis, and centralized incident monitoring. Use cases include:\nContinuous integration with built-in security validation.\nAutomated alerts and compliance reporting.\nReal-time visibility into vulnerabilities and code quality.\nPartner services focus on designing, implementing, and optimizing a DevSecOps pipeline that ensures secure, compliant, and efficient software delivery.\n1.2 PROJECT SUCCESS CRITERIA ≥95% of code commits pass automated security scans before deployment.\nReal-time alerts are sent within 2 minutes of anomaly detection.\nSecurity Hub compliance score ≥90%.\nSuccessful integration between CodePipeline, CodeBuild, and Security Hub with zero manual intervention.\n1.3 ASSUMPTIONS All AWS accounts are pre-configured with IAM roles and permissions for CodePipeline and CodeBuild.\nGitLab repository access and webhooks are enabled.\nSecurity tools (e.g., Trivy, Bandit, SonarQube) are available in CodeBuild environment.\nThe organization follows AWS Well-Architected and Security best practices.\n2. SOLUTION ARCHITECTURE / ARCHITECTURAL DIAGRAM 2.1 TECHNICAL ARCHITECTURE DIAGRAM The proposed solution integrates multiple AWS services for CI/CD, automated security analysis, and monitoring. It includes components for source control (GitLab), build and test automation (CodeBuild), pipeline orchestration (CodePipeline), AI-based code review (CodeGuru Reviewer), and centralized alerting (Security Hub + SNS).\nTools used: GitLab\nAWS CodePipeline\nAWS CodeBuild\nAWS CodeGuru Reviewer\nAWS Security Hub, GuardDuty, Detective\nSonarQube, Trivy, Bandit\n2.2 TECHNICAL PLAN The partner will develop buildspec scripts in YAML for CodeBuild to automate:\nSource code scanning (Trivy, Bandit)\nStatic code analysis (CodeGuru Reviewer)\nBuild packaging and deployment triggers\nAll deployments will be version-controlled via GitLab CI triggers. Configuration files will follow Infrastructure as Code principles using AWS CloudFormation.\n2.3 PROJECT PLAN The team will adopt Agile Scrum methodology over 4 sprints (2 weeks each). Stakeholders will participate in Sprint Reviews and Retrospectives.\nRoles and responsibilities:\nDevOps Engineer: CI/CD pipeline setup\nSecurity Engineer: Security integration and analysis\nProject Lead: Coordination, reporting, documentation\nWeekly sync-up meetings will be held via Slack and AWS Chime.\n2.4 SECURITY CONSIDERATIONS Access – IAM least privilege, MFA for admin accounts\nInfrastructure – Private subnets for build agents\nData – S3 encryption (SSE-KMS), CodeBuild log encryption\nDetection – GuardDuty and Security Hub continuous monitoring\nIncident Management – SNS notifications and CloudWatch alarms for anomalies\n3. ACTIVITIES AND DELIVERABLES 3.1 ACTIVITIES AND DELIVERABLES Project Phase Timeline Activities Deliverables/Milestones Total Man-days Assessment Week 1–2 Analyze existing CI/CD Report \u0026amp; Architecture Design 5 Setup base infrastructure Week 3–4 Create CodePipeline \u0026amp; CodeBuild Pipeline Deployed 7 Integrate Security Tools Week 5–6 Add SonarQube, Trivy, Bandit Security Scan Active 6 Monitoring Setup Week 7 Connect Security Hub, CloudWatch Alert System Operational 4 Testing \u0026amp; Go-Live Week 8 Final testing, documentation Go-Live Report 3 Handover Week 9 Knowledge transfer Final Project Handover 2 3.2 OUT OF SCOPE On-premises application security scanning\nThird-party compliance frameworks beyond AWS tools\nNon-AWS CI/CD environments\n3.3 PATH TO PRODUCTION The Proof-of-Concept focuses on AWS-native DevSecOps integration. For production deployment, additional steps such as multi-account security setup, network isolation, and automated patching will be required.\n4. EXPECTED AWS COST BREAKDOWN BY SERVICES Service Description Estimated Monthly Cost (USD) CodePipeline Orchestration 10 CodeBuild Build + Scan 30 CodeGuru Reviewer Code analysis 25 Security Hub Aggregation + Alerts 15 CloudWatch Logs + Metrics 10 S3 Artifact storage 5 SNS Notifications 5 Total (approx.) 100 USD/month 5. TEAM Name Student ID Email / Contact Lê Công Cảnh SE183750 canhlcse183750@fpt.edu.vn Phùng Gia Đức SE183187 ducpgse183187@fpt.edu.vn Vũ Nguyễn Bình SE193185 vunguyenbinh25@gmail.com Lê Minh Dương SE184079 duonglmse184079@fpt.edu.vn Nguyễn Phi Duy SE180529 duynpse180529@fpt.edu.vn 6. RESOURCES \u0026amp; COST ESTIMATES Resource Responsibility Rate (USD/hr) Total Hours Cost (USD) Solution Architect Design \u0026amp; Review 60 40 2400 DevOps Engineer Pipeline Implementation 45 60 2700 Security Engineer Tool Integration 50 50 2500 Total 150 7600 7. ACCEPTANCE Upon completion of each phase, the provider will submit deliverables to the customer with an Acceptance Form. The customer will review within 8 business days and provide either:\nWritten acceptance confirmation, or\nRejection notice with feedback.\nIf no response is received within the acceptance period, the deliverable is deemed accepted.\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop2/1-worklog/1.2-week2/",
	"title": "Week 2 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 2 Objectives: Complete all learning content in Module 01. Understand AWS global infrastructure and service management tools. Practice AWS Budgets for cost optimization. Complete IAM and Billing labs. Learn about AWS Support plans and how to open a support case. Tasks to be carried out this week: Day Task Start End Reference 2 Module 01-04: AWS Global InfrastructureModule 01-05: Service Management Tools 15/09 15/09 FCJ Module 01 3 Module 01-06: Cost OptimizationLab 07-01 → 07-03 16/09 16/09 FCJ Lab 07 4 Lab 07-04 → 07-06Create Cost/Usage/Savings Budget 17/09 17/09 AWS Billing 5 IAM Labs 01-01 → 01-04 18/09 18/09 FCJ Lab 01 6 Learn AWS Support PlansOpen a sample support case 19/09 19/09 AWS Support Docs Week 2 Achievements: Understood how AWS organizes regions, AZs, and infrastructure. Learned to manage cost forecasting with AWS Budgets. Completed all IAM foundational labs. Learned how AWS Support works and how to submit a support request. "
},
{
	"uri": "http://localhost:1313/fcj-workshop2/5-workshop/5.3-sonarqube-analysis/",
	"title": "Sonarqube-analysis",
	"tags": [],
	"description": "",
	"content": "SonarQube Analysis Architecture Sonar Scanner → SonarQube During the build phase, Sonar Scanner performs:\nCode quality analysis. Bug detection. Security vulnerability detection (Security Hotspots, Vulnerabilities). Code smell, complexity, duplication calculation. Results are sent to:\nSonarQube Server (EC2) SonarQube processes and saves the results to its database. Webhook Trigger When the analysis is complete:\nSonarQube makes an HTTP POST call to the API Gateway endpoint. Payload contains: Project name Quality Gate status (Pass/Fail) Error summary (bugs, vulnerabilities, code smells,…) "
},
{
	"uri": "http://localhost:1313/fcj-workshop2/4-eventparticipated/4.3-event3/",
	"title": "Event 3",
	"tags": [],
	"description": "",
	"content": "Summary Report: \u0026ldquo;AWS Well-Architected Security Pillar Workshop\u0026rdquo; Event Details Date: November 29, 2025 — Morning Only Time: 08:30 AM – 12:00 PM Location: AWS Vietnam Office Speakers Le Vu Xuan An – AWS Cloud Club Captain HCMUTE Tran Duc Anh – AWS Cloud Club Captain SGU Tran Doan Cong Ly – AWS Cloud Club Captain PTIT Danh Hoang Hieu Nghi – AWS Cloud Club Captain HUFLIT 8:30 – 8:50 AM | Opening \u0026amp; Security Foundation Role of the Security Pillar within the Well-Architected Framework Core principles: Least Privilege, Zero Trust, Defense in Depth Shared Responsibility Model Common cloud threats in Vietnam Pillar 1 — Identity \u0026amp; Access Management 8:50 – 9:30 AM | Modern IAM Architecture IAM basics: Users, Roles, Policies – avoiding long-term credentials IAM Identity Center: SSO and permission sets SCPs and permission boundaries for multi-account setups MFA, credential rotation, Access Analyzer Mini Demo: Validate IAM Policy + simulate access Pillar 2 — Detection 9:30 – 9:55 AM | Detection \u0026amp; Continuous Monitoring Organization-level CloudTrail, GuardDuty, Security Hub Logging across all layers: VPC Flow Logs, ALB Logs, S3 Access Logs Alerting \u0026amp; automation with EventBridge Detection-as-Code (infrastructure + rules as code) 9:55 – 10:10 AM | Coffee Break Pillar 3 — Infrastructure Protection 10:10 – 10:40 AM | Network \u0026amp; Workload Security VPC segmentation: private vs public placement Security Groups vs NACLs and when to use each WAF, Shield, Network Firewall Workload protection fundamentals: EC2, ECS/EKS Pillar 4 — Data Protection 10:40 – 11:10 AM | Encryption, Keys \u0026amp; Secrets KMS: key policies, grants, rotation Encryption at-rest \u0026amp; in-transit for S3, EBS, RDS, DynamoDB Secrets Manager \u0026amp; Parameter Store best practices Data classification and access guardrails Pillar 5 — Incident Response 11:10 – 11:40 AM | IR Playbook \u0026amp; Automation AWS Incident Response lifecycle Example playbooks: Compromised IAM key Public S3 exposure EC2 malware detection Snapshotting, isolation \u0026amp; evidence collection Automated response with Lambda / Step Functions 11:40 AM – 12:00 PM | Wrap-Up \u0026amp; Q\u0026amp;A Summary of all 5 pillars Common pitfalls \u0026amp; real examples from Vietnamese companies Recommended learning path (Security Specialty, SA Pro) "
},
{
	"uri": "http://localhost:1313/fcj-workshop2/3-blogstranslated/3.3-blog3/",
	"title": "Using AWS Service Reference Information to Automate Policy Management Workflows",
	"tags": [],
	"description": "",
	"content": "AWS provides a comprehensive service reference dataset that helps organizations manage their AWS service usage more securely and efficiently. This dataset includes detailed information about IAM permissions, data, APIs, supported actions, and conditions for every AWS service. Customers can use this dataset to automate the creation, review, and management of IAM policies.\nBelow is an overview of how AWS describes the use of this information to automate policy management workflows.\n1. Purpose of the AWS Service Reference Dataset The dataset helps organizations:\nAutomate IAM policy creation. Analyze usage to identify excessive permissions. Reduce over-privileged access following the least-privilege principle. Integrate into review, audit, or change-management workflows. The dataset includes:\nAll supported actions for each AWS service. IAM permissions required for those actions. The resource types each action applies to. Supported condition keys. 2. Core Usage Models Organizations can apply the AWS service reference dataset across multiple use cases:\nAutomated IAM Policy Generation Using CloudTrail data together with the dataset, organizations can:\nIdentify the actions an application actually performs. Generate just-in-time IAM policies based on real usage. Avoid granting unnecessary permissions. Policy Optimization Automated tooling can:\nDetect unused actions. Suggest removal of excessive permissions. Highlight differences between granted and used permissions. Assisting Accurate Policy Authoring The dataset provides:\nCorrect IAM action names. Supported resource types for each action. Applicable condition keys. This reduces errors when writing policies manually.\nAutomating Policy Review and Approval Organizations can use the dataset to:\nValidate policies before approval. Check whether an IAM action is valid. Automatically reject policies requesting unnecessary permissions. 3. Available Data in the Reference Dataset The dataset includes:\nAWS service list. IAM action sets for each service. Supported resource types for each action. Allowed condition keys. Documentation mappings for permissions and APIs. AWS updates the dataset monthly.\n4. Automating Policy Management Workflows A complete system may combine:\nCloudTrail (real API usage logs). The IAM service reference dataset (to validate actions and resources). A policy review and approval pipeline. Typical workflow:\nCollect API usage from CloudTrail. Compare against IAM reference data to determine required permissions. Generate or suggest policy updates. Submit the policy to an approval pipeline. Automatically apply approved policy changes. 5. Key Benefits Stronger security through least-privilege access. Automated workflows reduce manual effort. Lower risk of human errors. Better visibility into permission changes. Smooth integration with DevOps processes. 6. Conclusion The AWS service reference dataset provides a foundational resource enabling organizations to automate IAM policy management. Combined with CloudTrail and workflow automation, it allows teams to build precise, secure, and intelligent policy systems with reduced risk and improved governance.\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop2/1-worklog/1.3-week3/",
	"title": "Week 3 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 3 Objectives: Understand AWS networking fundamentals: VPC, Subnets, Route Tables. Perform VPC-related labs in Module 02. Learn the differences between Security Groups and NACLs. Tasks to be carried out this week: Day Task Start End Reference 2 Watch Module 02-01 → 02-02Review VPC theory 22/09 22/09 FCJ Module 02 3 Create VPC and SubnetsConfigure Route Tables 23/09 23/09 Lab 03 4 Create Internet GatewayCreate NAT Gateway 24/09 24/09 VPC Labs 5 Configure Security GroupsCompare SG vs NACL 25/09 25/09 Module 02 6 Test connectivity between subnets (ping/SSH) 26/09 26/09 Lab Test Week 3 Achievements: Understood the structure of AWS networking. Created a fully functional VPC setup. Learned when to use SGs and NACLs appropriately. "
},
{
	"uri": "http://localhost:1313/fcj-workshop2/3-blogstranslated/",
	"title": "Translated Blogs",
	"tags": [],
	"description": "",
	"content": "This section will list and introduce the blogs you have translated. For example:\nBlog 1 - Using Large Language Models on Amazon Bedrock for Multi-step Task Execution Blog 2 - Updated Carbon Methodology for the AWS Customer Carbon Footprint Tool (CCFT) Blog 3 - Using AWS Service Reference Information to Automate Policy Management Workflows "
},
{
	"uri": "http://localhost:1313/fcj-workshop2/5-workshop/5.4-lambda-and-notification/",
	"title": "Lambda-and-notification",
	"tags": [],
	"description": "",
	"content": "Lambda Processing + Notification API Gateway → Lambda API Gateway receives webhook from SonarQube and passes payload to Lambda.\nLambda does:\nParse data from SonarQube. Format message (project, error type, severity). Send content to SNS topic. SNS Notifications SNS sends email to dev team:\nReport security errors. Report code errors. Report Quality Gate failures. Link directly to SonarQube dashboard. Benefits:\nDev knows errors immediately. Reduced debugging time. Increase software quality and safety. "
},
{
	"uri": "http://localhost:1313/fcj-workshop2/1-worklog/1.4-week4/",
	"title": "Week 4 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 4 Objectives: Learn EC2 concepts: Instances, AMI, EBS, Metadata. Practice launching EC2 and attaching EBS. Understand Autoscaling and Load Balancing. Perform EC2 backup using AWS Backup. Tasks to be carried out this week: Day Task Start End Reference 2 Watch Module 03-01EC2 types, pricing, AMI 29/09 29/09 Module 03 3 Create EC2 + EBSUse User Data, Metadata 30/09 30/09 EC2 Docs 4 Create Load BalancerSet up Autoscaling Group 01/10 01/10 EC2 Labs 5 Perform AWS Backup (Lab13)Create backup plan 02/10 02/10 AWS Backup 6 Restore EC2 from backup 03/10 03/10 Backup Lab Week 4 Achievements: Learned full EC2 workflow. Implemented Load Balancer + Auto Scaling. Successfully backed up and restored an EC2 instance. "
},
{
	"uri": "http://localhost:1313/fcj-workshop2/4-eventparticipated/",
	"title": "Events Participated",
	"tags": [],
	"description": "",
	"content": " In this section, you should list and describe in detail the events you have participated in during your internship or work experience.\nEach event should be presented in the format Event 1, Event 2, Event 3…, along with the following details:\nEvent name Date and time Location (if applicable) Your role in the event (attendee, event support, speaker, etc.) A brief description of the event’s content and main activities Outcomes or value gained (lessons learned, new skills, contribution to the team/project) This listing helps demonstrate your actual participation as well as the soft skills and experience you have gained from each event. During my internship, I participated in two events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: AI/ML/GenAI on AWS\nDate \u0026amp; Time: 8:30 AM – 12:00 PM, Saturday, November 15, 2025\nLocation: 36th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: DevOps on AWS\nDate \u0026amp; Time: 8:30 AM – 5:00 PM, Monday, November 17, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 3 Event Name: AWS Well-Architected Security Pillar Workshop Date \u0026amp; Time: 08:30 AM – 12:00 PM, November 29, 2025 — Morning Only\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop2/5-workshop/5.5-closed-loop-devsecops/",
	"title": "Closed-loop-devsecops",
	"tags": [],
	"description": "",
	"content": "DevSecOps Feedback Loop (Closed Cycle) The system creates a closed DevSecOps loop: Dev commits code → GitLab ↓ CodePipeline runs → CodeBuild scans → SonarQube analyzes ↓ Webhook → Lambda → SNS sends email ↓ Dev receives error → Fix → Commit again ↓ Return to loop\nMain benefits Automatically detects bugs and security vulnerabilities. Real-time alerts help devs fix immediately. Ensures code quality throughout the development lifecycle. DevSecOps culture in practice: “Security shift-left”. Conclusion Pipeline ensures a modern, automated, secure and sustainable development process. Each commit is rigorously analyzed, helping the dev team maintain high quality standards.\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop2/5-workshop/",
	"title": "Workshop",
	"tags": [],
	"description": "",
	"content": "DevSecOps Security Scan Pipeline on AWS Overview In this workshop, we build a DevSecOps pipeline on AWS to integrate security right into the CI/CD process instead of manually handling it at the end of the project. Each time a developer pushes code to the repository, the system will automatically trigger security and source code quality checks.\nMain objectives Automatically scan for security vulnerabilities after each commit/push of code. Perform container image and dependency scanning using Trivy, helping to detect vulnerabilities in libraries and base images. Run Bandit to check for security issues in Python/JavaScript code. Integrate SonarQube to evaluate code quality (code smells, duplication, coverage, maintainability\u0026hellip;). Push important findings to AWS Security Hub to centrally manage security alerts. Send real-time notifications (via Email/SNS/Chat\u0026hellip;) when detecting serious problems for the development team to handle promptly.\nContent Workshop overview Pipline flow Sonarqube analysis Lambda and notification Close loop devsecops "
},
{
	"uri": "http://localhost:1313/fcj-workshop2/1-worklog/1.5-week5/",
	"title": "Week 5 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 5 Objectives: Master S3 bucket operations: ACL, versioning, replication. Host a static website using S3. Use CloudFront for CDN distribution. Tasks to be carried out this week: Day Task Start End Reference 2 Module 04-02: S3 basicsCreate S3 bucket 06/10 06/10 S3 Docs 3 Enable Static Website HostingManage Public Access Block 07/10 07/10 S3 Lab 4 Create CloudFront distribution 08/10 08/10 CloudFront Docs 5 Enable VersioningConfigure Cross-region Replication 09/10 09/10 Advanced S3 6 Test website + CloudFront routing 10/10 10/10 Lab Test Week 5 Achievements: Mastered S3 bucket configuration. Built a complete static website hosting solution. Distributed content via CloudFront successfully. "
},
{
	"uri": "http://localhost:1313/fcj-workshop2/1-worklog/1.6-week6/",
	"title": "Week 6 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 6 Objectives: Learn IAM advanced concepts: Role, Policy, Access Control. Practice IAM Role creation for EC2 and Lambda. Explore AWS Security Hub and GuardDuty. Tasks to be carried out this week: Day Task Start End Reference 2 Module 05-01 → 05-03: IAM, Cognito 13/10 13/10 Module 05 3 Create IAM Roles (EC2/Lambda)Inline vs Managed Policy 14/10 14/10 IAM Docs 4 Practice Switch RoleUse IAM Policy Simulator 15/10 15/10 IAM Tools 5 Enable Security HubEnable GuardDuty 16/10 16/10 AWS Security 6 Review all Security fundamentals 17/10 17/10 Notes Week 6 Achievements: Understood IAM deep concepts. Created and tested IAM Roles correctly. Enabled and analyzed GuardDuty \u0026amp; Security Hub findings. "
},
{
	"uri": "http://localhost:1313/fcj-workshop2/6-self-evaluation/",
	"title": "Self-Assessment",
	"tags": [],
	"description": "",
	"content": "During my internship at Amazon Web Services Vietnam Co., Ltd. from September 8th, 2025 to December 12th, 2025, I had the opportunity to apply what I learned in school to real-world tasks and observe how large-scale cloud environments operate.\nBy joining the First Cloud Journey program, I improved my skills in communication, teamwork, time management, and gained deeper knowledge in AWS services and cloud practices.\nThroughout the internship, I consistently aimed to complete my tasks on time, follow company guidelines, and collaborate actively with my teammates and mentors.\nBelow is my self-evaluation:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Ability to apply AWS/Cloud concepts, work quality, tool proficiency ✅ ☐ ☐ 2 Ability to learn Speed of learning, adaptability, willingness to explore new technologies ☐ ✅ ☐ 3 Proactiveness Taking initiative, not waiting passively for instructions ✅ ☐ ☐ 4 Responsibility Completing tasks on time and ensuring accuracy ✅ ☐ ☐ 5 Discipline Following schedules, procedures, and workplace rules ☐ ✅ ☐ 6 Growth mindset Accepting feedback and continuously improving ✅ ☐ ☐ 7 Communication Clear expression of ideas and progress ☐ ✅ ☐ 8 Teamwork Ability to collaborate effectively with the team ✅ ☐ ☐ 9 Professional behavior Respectful attitude and proper workplace conduct ✅ ☐ ☐ 10 Problem-solving skills Ability to analyze issues and propose practical solutions ☐ ✅ ☐ 11 Contribution to project/team Completing assigned tasks, supporting team progress ☐ ✅ ☐ 12 Overall performance General evaluation of the internship ☐ ✅ ☐ Areas for improvement: Strengthen personal discipline and consistency in work habits. Improve analytical thinking and approach to problem-solving. Handle technical issues more quickly and safely. Set clearer goals and plan work more effectively. "
},
{
	"uri": "http://localhost:1313/fcj-workshop2/1-worklog/1.7-week7/",
	"title": "Week 7 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 7 Objectives: Learn the basics of database services on AWS. Get familiar with Amazon RDS, Aurora, DynamoDB. Understand backup, restore, and database security. Complete Module 06 labs. Tasks to be carried out this week: Day Task Start End Reference 2 Watch Module 06-01 → 06-02Learn RDS \u0026amp; Aurora basics 20/10 20/10 Module 06 3 Create RDS Subnet GroupCreate DB Security Group 21/10 21/10 RDS Lab 4 Launch RDS instanceTest connection from EC2 22/10 22/10 Lab 05 5 Perform DB backup \u0026amp; restoreReview snapshot usage 23/10 23/10 RDS Docs 6 Study DynamoDB \u0026amp; ElastiCache 24/10 24/10 NoSQL Docs Week 7 Achievements: Understood SQL vs NoSQL differences on AWS. Successfully created and connected to an RDS database. Learned how to configure DB backup, snapshot, and restore. Gained basic understanding of DynamoDB and ElastiCache. "
},
{
	"uri": "http://localhost:1313/fcj-workshop2/7-feedback/",
	"title": "Sharing and Feedback",
	"tags": [],
	"description": "",
	"content": "Overall Evaluation 1. Working Environment\nThe working environment at FCJ is comfortable and easy to adapt to. Everyone is supportive and willing to help whenever I run into difficulties, even outside learning hours. The workspace is organized and quiet, which helps me stay focused. It would be even better if we had more team bonding sessions to get to know each other more.\n2. Support from Mentor / Team Admin\nMy mentor provides clear explanations and guides me step-by-step while still giving me space to try solving things on my own first. The admin team is very helpful with documents, logistics, and creating a smooth experience throughout the internship. I really appreciate that my mentor encourages me to ask questions instead of being afraid of making mistakes.\n3. Relevance to My Major\nMost of the tasks match the subjects I studied at university, and I was also exposed to many new topics such as cloud operations and DevOps practices. This mixture helped me reinforce my foundation while gaining hands-on experience.\n4. Learning \u0026amp; Skill Development Opportunities\nDuring this internship, I learned how to use new technical tools, improved my teamwork ability, and practiced professional communication. My mentor also shared real-world insights that helped me understand more clearly how the cloud industry works and what skills I need moving forward.\n5. Team Culture \u0026amp; Collaboration\nThe culture in FCJ is very positive. People respect each other and keep a friendly atmosphere even when the workload increases. I always felt included as part of the team, despite being an intern.\n6. Internship Policies \u0026amp; Benefits\nAlthough there is no internship allowance, the flexible scheduling and chances to join internal workshops are very valuable to me.\nAdditional Questions What did you find most satisfying during your internship?\nBeing guided closely by mentors and receiving clear feedback on how to improve. What should the company improve for future interns?\nAt the moment, I don’t have any suggestions. Would you recommend your friends to intern here? Why?\nYes. The FCJ program is extremely helpful and provides solid knowledge for future career development. Suggestions \u0026amp; Expectations Any suggestions to improve the internship experience?\nI hope AWS will allow interns to visit the office more frequently. Would you like to continue this program in the future?\nYes, because AWS is a great place to grow professionally and offers many opportunities in cloud-related career paths. Any other comments?\nNothing additional, except my appreciation to my mentor and the FCJ team for all their support throughout the internship. "
},
{
	"uri": "http://localhost:1313/fcj-workshop2/1-worklog/1.8-week8/",
	"title": "Week 8 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 8 Objectives: Kick off the “Security Scan Pipeline on AWS Cloud” project. Review project requirements and define team roles. Analyze the architecture of a DevSecOps pipeline. Prepare the GitHub repository and initial documentation. Tasks to be carried out this week: Day Task Start End Reference 2 Team meeting to assign rolesReview and analyze project proposal 27/10 27/10 Project Docs 3 Study DevSecOps pipeline architectureReview CodePipeline, CodeBuild, CodeDeploy 28/10 28/10 AWS DevOps Docs 4 Create the initial architecture draft 29/10 29/10 Diagram Tools 5 Create GitHub repoAssign permissions to members 30/10 30/10 GitHub 6 Review buildspec.yml \u0026amp; appspec.ymlPrepare needed IAM permissions 31/10 31/10 CI/CD Docs Week 8 Achievements: Fully understood project scope and requirements. Completed the first draft of the pipeline architecture. Set up GitHub repo and prepared required documentation. "
},
{
	"uri": "http://localhost:1313/fcj-workshop2/1-worklog/1.9-week9/",
	"title": "Week 9 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 9 Objectives: Finalize the design of the Security Scan Pipeline. Build the foundational AWS infrastructure. Connect GitHub to AWS CodePipeline. Tasks this week: Day Task Start End Reference 2 Finalize pipeline architecture diagram 03/11 03/11 Project Docs 3 Create required IAM RolesCodeBuildRole, CodeDeployRole 04/11 04/11 IAM Docs 4 Create S3 Bucket for artifactsEnable encryption 05/11 05/11 S3 Docs 5 Launch EC2 instance for deploymentConfigure security groups 06/11 06/11 EC2 Docs 6 Connect GitHub → CodePipelineTest Source stage 07/11 07/11 GitHub Docs Week 9 Achievements: Pipeline architecture finalized. Core infrastructure (IAM, S3, EC2) successfully deployed. GitHub integrated with CodePipeline without issues. "
},
{
	"uri": "http://localhost:1313/fcj-workshop2/1-worklog/1.10-week10/",
	"title": "Week 10 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 10 Objectives: Build the CI/CD pipeline structure. Integrate CodeGuru Reviewer for automated security scanning. Create buildspec.yml and appspec.yml. Tasks this week: Day Task Start End Reference 2 Create CodePipeline: Source → Build → Scan → Deploy 10/11 10/11 CodePipeline Docs 3 Configure CodeBuildWrite buildspec.yml 11/11 11/11 CodeBuild Docs 4 Integrate CodeGuru Reviewer 12/11 12/11 CodeGuru Docs 5 Create appspec.yml for deployment 13/11 13/11 CodeDeploy Docs 6 Run pipeline test — fix IAM permission errors 14/11 14/11 Troubleshooting Notes Week 10 Achievements: Pipeline functional through Source, Build, and Scan stages. CodeGuru successfully scanning code for security issues. Deployment configuration ready for Week 11. "
},
{
	"uri": "http://localhost:1313/fcj-workshop2/1-worklog/1.11-week11/",
	"title": "Week 11 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 11 Objectives: Complete the Deploy stage. Perform full pipeline testing. Fix issues and improve workflow stability. Tasks this week: Day Task Start End Reference 2 Configure CodeDeploy Application \u0026amp; Deployment Group 17/11 17/11 CodeDeploy Docs 3 Review AppSpec lifecycle hooks 18/11 18/11 AppSpec Docs 4 First end-to-end pipeline test → EC2 permission error 19/11 19/11 Test Logs 5 Fix IAM Role \u0026amp; adjust appspec.yml 20/11 20/11 Troubleshooting 6 Enable CloudWatch, GuardDuty, Security Hub 21/11 21/11 AWS Security Docs Week 11 Achievements: Successfully deployed application to EC2 via CodeDeploy. Full pipeline now running smoothly end-to-end. Security monitoring tools activated and functioning. "
},
{
	"uri": "http://localhost:1313/fcj-workshop2/1-worklog/1.12-week12/",
	"title": "Week 12 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 12 Objectives: Complete all project documentation. Gather logs, test results, and error reports. Prepare the presentation slides. Finalize the internship report. Tasks this week: Day Task Start End Reference 2 Collect pipeline logs from CloudWatch 24/11 24/11 CloudWatch 3 Write pipeline operation guide 25/11 25/11 Project Docs 4 Prepare handover package (architecture, configs, diagrams) 26/11 26/11 Deliverables 5 Document errors and troubleshooting steps 27/11 27/11 Troubleshooting Notes 6 Create final presentation slidesReview all worklogs from Week 1–12 28/11 28/11 Final Review Week 12 Achievements: Completed all technical documentation for the project. Prepared full test logs and error reports. Pipeline is ready for final demonstration. Internship report finalized. "
},
{
	"uri": "http://localhost:1313/fcj-workshop2/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost:1313/fcj-workshop2/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]